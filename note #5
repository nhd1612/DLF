Optimizer 밑 데이터 전처리에 대해서 
Optimizer 클래스는 
매게 변수 갱신을 위한 기반 클래스이다. 
또한 구체적인 최적화 기법은 Optimizer 클래스를 상속한 자식클래스에서 구현한다. 
초기화 메서드는 target과 hooks라는 두개의 인스턴스 변수를 초기화한다. 
구체적인 매개변수 갱신은  update_one 메서드에서 수행, 자식 클래서에서 재정의 된다. 
또한 전처리는 add_hook 메서드를 사용하여 전처리를 진행한다.
식은 
class Optimizer:
  def init (self):
    self.target None
    selt.hooks []

  def setup(self, target):
    self target target
    return self

  def update(self):
    params = [p for p in self.target.params() if p.grad is not None]
    for f in self.hooks:
      f(params)

    for param in params:
      self.update_one(param)

  def update_one(self, param):
    raise NotimplementedError()

  def add_hook(self, f):
    self.hooks.append(f)
SGD 클래스 구현은
경사하강법으로 매개변수를 갱신하는 클랴스를 구현합니다.
SGD 클래스는 Optimizer 클래스를 상속합니다. 
또한 __init__메서드는 학습률을 받아 초기화를 진행한다. 
update_one 메서드에서 매개변수 갱신 코드를 구현할 수 있다. 
구현식은 
class SGD(Optimizer):
  def __init__(self, Ir=0.01):
    super().__init__()
    self.Ir = Ir
 
  def update_one(self, param):
    param.data = self.Ir param.grad.data
SGD 클래스를 사용하여 회귀 문제를 풀려면 
MLP 클래스를 사용하여 모델을 생성하고
SGD클래스로 매개 변수를 갱신한다. optimizer.update()를 사용한다.
다음은 기울기를 이용한 최적화 기법이다. 
Momentum, AdaGrad, AdaDelta, Adam이 있으며
OPtimizer 클래스를 이용해 다양한 최적화 기법을 필요에 따라 손쉽게 전환한다. 
Optimizer 클래스를 상속하여 다양한 최적화 기법을 구현이 가능하다. 
momentum 기법이란 
W는 갱신할 가중치 매개 변수이며, 𝜕𝐿/𝜕W는 기울기. n은 학습률을 말한다. 
v는 물리에서 말하는 속도에 해당하며, av는 물체가 아무런 힘을 받지 않을때 서서히 감속되는 역활을 한다.
MomentumSGD 구현 코드는 
속도에 해당하는 데이터, 딕셔너리 타입의 인스턴수 변수 self.vs에 유지가 되는 것이며 
초기화 시에는 vs에 아무것도 담겨 있지 않다. 
또한 Update_one()이 처음 호출될 때 매개변수와 같은 타입의 데이터를 생성한다. 
구현한 학습코드에서 손쉽게 Momentum으로 전환 되는 것을 볼 수 있다. 
class MomentumSGD(Optimizer):
  def __init__(self. Ir=0.01. momentum = 0.9):
    super().__init__()
    self.Ir = Ir
    self.momentum = momentum
    self.vs = {}
  
  def update_one(self, param):
    v_key id(param)
    if v_key not in self.vs:
      xp = cuda.get_array_module(param.data) 
      self.vs[v_key] xp.zeros_like (param.data)
    
    v = self.vs[v_key]
    v *= self.momentum
    v self.Ir param.grad.data
    param.data += v
이곳에서 optimizer = MomentumSGD(lr)로 변경하면 된다. 
다중클래스 분류는
여러 클래스로 분류하는 문제이다.
또한 분류 대상이 여러  가지 클래스 중 어디에 속하는지 추정한다.
get_item 함수는
variable의 다차원 배열 중에서 일부를 슬라이스하여 뽑아낸다.
(2,3)형상의 x에서 1번째 행의 원소를 추출한다. 
DeZero함수로 구현했기에역전파도 진행한다.
슬라이스 조작함수에서 슬라이스는
다차원배열의 일부를 추출하는 작업이다.
get_item 함수 사용 시 같은 인덱스를 반복 지정하여 동이랗ㄴ원소를 여러 번 빼낼 수 있다. 
그리고 
특수 메서드 설정이다.
get_item 함수를Variable 의 메서드로사용한다. 
그리고 슬라이스의 역전파도 이루어 진다.
신경망으로 다중클래스를 분류하는 법은
선형 회귀 때 이욯나신경망을그대로 사용할수 있다. 또한
MLP클래스로 구현해둔 신경망을 그대로 사용할 수 있다. 
아래는 입력 데이터의 차원수가 2개이고 3개의 클래스 분류하는 문제이다. 
from dezero.models import MLP
model = MLP((10,3))으로 되며 
2층으로 이루어진 완경 연결 신경망을 만들어준다. 
첫 번째 완전 연결 계층의 출력 크기는 10, 두 번째 완전연결 계층의 출력 크기는 3이다. 
model은 입력데이터를 3차원 벡터로 변환하는 법이다. 예시는
x = np.array([[0.2, -0.4]])
y = model(x)
print(y)
이다 
이를 해석해보면 
x의 현상은 (1,2), 샘플 데이터가 하나 있고 그 데이터는원소가 2개인 2차원 벡터이다. 
그리고 신경망의 출력 형태는(1,3) 이고 하나의 샘플데이터가 3개의 원소로 변환한다. 
(0번, 1번, 2번 원소 중)2번 원소가 0.92401356 으로 가장 크기때문에 2번 클래스로 분류한다.
다음은 소프트 맥스 함수이다. 
이는 신경망출력이 단순한 수치인데 
이 수치를 확률로 변환한다.
교차 엔트로피 오차는 다중 클래스 분류에적합한 손실 함수이다.
정답 데이터의 원소는 정답에 해당하는클래스면 1으로 기록하고 그렇지 않으면0으로 기록한다.  
이러한 표현방식을 원핫 벡터라고 한다.
정답 클래스에 해당하는 번호의 확률 p를 추출함으로써 교차 엔트로피의 오차를 계산한다. 
p[t]는 벡터 t번째 요소만을 추출한다. 
교차 엔트로피 오차 구현을 진행해 보면
인수 x는 신경망에서 소프트 맥스적용하기 전의 출력을 한다
clip함수는 x_min 으로 변환 후 x_max 이상이면 변환해준다. 
np_arrange(N)은 [0,1,...,N-1]형태의ndattay인스턴스를 생성한다.  
다중 클래스 분류 수행은 
소프트맥스 함수와 교차 엔트로피 오차를 구현하고
스파이럴데이터셋 이라는작은 나선형 데이터 셋을 사용하여 다중 클래스 분류를 시행한다.
get_spiral 함수를 사용하여 데이터 셋 읽어온다. 
그리고 train = True 일시 학습 데이터 반환하고 False는 테스트 데이터 반환한다.
딥러닝의 특징은 층을 더 깊게 쌓는 식으로 표현력을 키울 수 있다. 
대규모 데이터 셋을 처리할려면
거대한 데이터를 하나의 mdarray인스턴스로 처리하면모든 원소를 한꺼번에 메모리를올린다.
대규모 데이터를 처리할 수 있도록 데이터 셋 전용 클래스 데이터셋을제작한다.
dataset클래스는 기반 클래스의 역활을한다.
prepare 메서드가 데이터 준지 작업을 하도록 구현해준다.
__getitem__메서드는 단순히지정된 인덱스에 위치하는 데이터를 꺼낸다.
__len__메서드는 데이터셋의 길이를 알려준다.
작은 데이터 셋은 dataset클래스의 인스턴스 변수인 data와label에 직접 ndarray 인스턴스를 유지해도 무리가 없다.
큰 데이터셋의 구현방식은 지금의 방식을 사용할 수 없다. 
신경망으로 학습시킬 때는데이터셋중 일부를 미니배치로꺼낸다.
인덱스를 지정하여 batch에 여러 데이터가 리스트로 저장된다.
sprial클래스 사용 시 미니배치를 만드는 부분의 코드를 수정한다. 
dataset클래스를 사용하여 신경망 학습을 진행하는데
이점은 다른 데이터셋으로 교체해 학습할때 확인을 할 수 있다.
데이터셋의 전처리는
모델에 데이터를입력하기전에데이터를 특정한 형태로 가공할 경우가 많다.
데이터 형상 변형과 데이터 확장 등등이 진행되고 
초기화 시에는 transform과 target_transform을 새롭게 받는다. 
transform 은 입력 데이터 하나에 대한 변환을 처리하고, target_transform 레이블 하나에 대한 변환을 처리한다.

Attention
seq2seq의 문제는
Encoder의 출력은 고정 길이 벡터이다.
또한 고정길이 벡터는 입력 문장의 길이에관계없이항상 같은 길이의 벡터로 변환한다.
이것을 개선하기 위해선
1. LSTM 계층의 마지막 은닉 상태만을 Decoder에 전달한다.
2. Encoder의 출력 길이는 입력 문장에 따라 바꿔주도록 한다.
3. 은닉 상태 벡터를 모두 이용하면 입력된 단어와 같은 수의 벡터를 얻는다.
인코더를 출력하는 hs 행렬은 
