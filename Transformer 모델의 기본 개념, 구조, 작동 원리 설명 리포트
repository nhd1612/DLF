Transformer 모델의 기본 개념, 구조, 작동 원리 설명 리포트

딥러닝 프레임워크[00]
최인엽 교수님 

201904236 산업데이터사이언스학부 전병준
202184042 데이터 사이언스 학과 이재성

1. 서론
Transformer 모델의 소개

Attention은 2015년에 나왔지만 이 기법을 적용하고 좀더 높은 성능으로 업그레이드하여 transformer라는 이름으로 2017년에 나왔고,
현재 자연어처리 분야를 넘어서 인공지능의 많은 분야에 인용되고 있습니다.
기존의 RNN이나 LSTM과 같은 seq2seq 방식은 하나의 벡터에 모두 압축해야 했고, 
이로 인해 병목현상이 발생한다는 단점이 있었습니다. 
따라서 입력 시퀀스 전체에서 정보를 추출하여 병렬연산을 하는 방식으로 발전하게 되었고,
인코더-디코더 구조를 통해 시퀀스 데이터를 효과적으로 처리하는 transformer가 나오게 되었습니다.

Transformer 모델이 왜 중요한지, NLP 및 기타 분야에서의 역할
RNN을 사용하지 않는 기계변역이기 때문에 학습이 빠르고 성능이 빠르기 때문이다. Transformer모델은 다양한 NLP모델들에서 사용됩니다.
BERT - 트랜스포머가 갖는 인코더만 사용하여 진행하는 모델 이는 단어 인베딩과 문서인베딩과 분류작업, Q&N 에서 사용된다.
GPT - 생성 모형이며 트랜스포머가 갖는 디코더만 사용하며, 새로운 텍스트를 생성하는 목적으로 사용된다. 
BART - 인코더와 디코더를 사용하여 텍스트 서머라이즈의 목적으로 사용된다.
추가적으로 컴퓨터 비전, 음성 처리, 게임 AI등 다양한 분야에서 응용이 되고 있다. 
