Transformer 모델의 기본 개념, 구조, 작동 원리 설명 리포트

딥러닝 프레임워크[00]
최인엽 교수님 

201904236 산업데이터사이언스학부 전병준
202184042 데이터 사이언스 학과 이재성

1. 서론

Transformer 모델의 소개

Attention은 2015년에 나왔지만 이 기법을 적용하고 좀더 높은 성능으로 업그레이드하여 transformer라는 이름으로 2017년에 나왔고,
현재 자연어처리 분야를 넘어서 인공지능의 많은 분야에 인용되고 있다.
기존의 RNN이나 LSTM과 같은 seq2seq 방식은 하나의 벡터에 모두 압축해야 했고, 
이로 인해 병목현상이 발생한다는 단점이 있었다. 
따라서 입력 시퀀스 전체에서 정보를 추출하여 병렬연산을 하는 방식으로 발전하게 되었고,
인코더-디코더 구조를 통해 시퀀스 데이터를 효과적으로 처리하는 transformer가 나오게 되었다.

Transformer 모델이 왜 중요한지, NLP 및 기타 분야에서의 역할

RNN을 사용하지 않는 기계변역이기 때문에 학습이 빠르고 성능이 빠르기 때문이다.
또한 Transformer모델은 NLP모델 분야에서 중요한 역활을 담당하고 있다.
Transformer모델은 다양한 NLP모델들에서 사용된다. 
BERT - 트랜스포머가 갖는 인코더만 사용하여 진행하는 모델 이는 단어 인베딩과 문서인베딩과 분류작업, Q&N 에서 사용된다.
GPT - 생성 모형이며 트랜스포머가 갖는 디코더만 사용하며, 새로운 텍스트를 생성하는 목적으로 사용된다. 
BART - 인코더와 디코더를 사용하여 텍스트 서머라이즈의 목적으로 사용된다.
추가적으로 컴퓨터 비전, 음성 처리, 게임 AI등 다양한 분야에서 응용이 되고 있다. 

2. 이론적 배경 

self Attention

각각의 임베딩된 벡터가 self attention에 input으로 들어오면 output은 input과 같은 dimension의 벡터를 내보냅니다. 
그리고 output으로 나온 벡터들은 Neural Network에 각각 들어가게 됩니다.
임베딩된 벡터가 attention에 들어오면 각각의 query weight, key weight, value weight와 연산을 해서 각각의 query, key, value vector를 구할 수 있습니다.
그러면, 찾고자 하는 단어의 query vector를 가지고 해당 문장의 모든 key와 전부 내적을 합니다. 
같은 벡터일수록 내적하면 큰 값이 나오기 때문에 내적했을 때의 가장 큰 값이 쿼리와 유사한 벡터가 됩니다.
따라서 query와 key vector를 내적해서 score를 구합니다.
쿼리가 자기자신의 키 벡터와는 매우 유사하므로 스코어가 크고, 다른 단어의 키와 연산한 스코어는 비교적 낮을 것입니다.
이후, 해당하는 dimension의 루트값으로 나눠줍니다. 이 과정은 정규화를 위한 과정인데, 벡터값이 너무 큰경우 정규화를 통해 기울기를 안정적으로 해주기 위한 과정입니다.
그리고 query, key 말고도 value가 있었는데 이 모든 임베딩에서 나온 softmax값과 value를 전부 곱해줍니다. 그 값을모두 더해주면 output으로 벡터가 나오게 됩니다.
이것을 여러번 수행하면 Multi Head Attention입니다. 

Positional Encoding

먼저 단어들은 워드 임베딩을 통해 각각의 벡터로 변환됩니다. 
그리고 각각의 벡터들이 한꺼번에 들어가므로 위치정보가 없는 상태입니다. 
따라서, 위치값을 저장해주기 위해 positional encoding을 함께 수행해줍니다.
다시말해, 임베딩된 벡터에 positional encoding vector를 더해주어 최종적으로 input으로 들어갈 벡터를 생성하게 됩니다.
예를 들어 추가적인 설명을 드리겠습니다. 'This is my car'이라는 문장이 주어졌을 때, 문장을 구성하는 각각의 단어는
그에 상응하는 인덱스 값에 매칭이 되고, 이 인덱스 값들은 Input Embedding

