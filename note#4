7주차
텐서 
지금까지 다룬 변수 - 변수로 스칼라를 사용한다. 
텐서 사용은 
1. 머신러닝 데이터로 벡터나 행렬 등의 텐서가 주로 사용된다. 
2. 텐서 사용시으 주의점 파악과 DeZero 확장을 준비한다. 
3. 지금까지 구현한 DeZero함수들이 텐서도 다룰 수 있다는 것을 보여준다. 
DeZero에서 구현한 함수는 
add/mul/div/sin 등등 을 사용해봤고
입력과 출력이 모드 수칼라라고 가정한다. 
텐서의 처리는 
x가 텐서일 경우 -sin 함수가 원소별로 적용된다. 
이는 입력과 출력 텐서의 형상은 바뀌지 않는다. 
import numpy as np 
import dezero.functions as F 
from dezero import Variable

x = Variable(np.array([1, 2, 3], [4, 5, 6]]))
y = F.sin(x)
print(y)를 하면
variable([[ 0.84147098 0.90929743 0.14112001]
[-0.7568025 -0.95892427 -0.2794155]])라는 결과 값이 나오며 

x = Variable (np.array([[1.2.3]. [4.5.6]]))
c = Variable(np.array([[10, 20, 30], [40, 50, 60]]))
y = x + c
print(y)
를 하면 
variable([[11 22 33]
[44 55 66]])라는 결고 값이 나온다. 
구현한 함수들이 텐서를 이용해 계산해도 역전파 코드가 문제없이 동작하는 것을 배운다. 
1. 스칼라를 대상으로 역전파를 구현하고 
2. 텐서의 원소별 스칼라 계산이 이루어지면, 스칼라를 가정해 구현한 역전파는 텐서의 원소별 계산에도 성립한다.
원소별 계산을 수행하는 DeZero 함수는 
텐서를 사용한 계산에도 역전파를 올바르게 해낼 것을 유추 할 수 있다. 
그리고 sum함수를 사용하면 주어진 텐서에의 모든 원소의 총합을 구해 하나의 스칼라로 출력한다. 
코드는
x = Variable(np.array([[1.2.3]. [4, 5, 6]]))
c = Variable(np.array([[10, 20, 30], [40,50,60]]))
t = x + c
y = F.sum(t)
print(y)
하면 결과값은 
variable(231)으로 나온다. 
마지막 출력이 스칼라인 계산 그래프에 대한 역전파
y.backward(retain_grad=True)를 실행하면 각 변수의 미분값이 구해지고
기울기의 형상과 순전파 때의 데이터의 형상이 일치한다.(x.shape == x.grad.shape, c.shape == c.grad.shape)
y.backward(retain_grad=True)
print(y.grad)
print(t.grad)
print(x.grad)
print(c.grad)
을 기입하면 
variable(1)
variable([[111]
          [111]])
variable([[111]
          [111]])
variable([[111]
          [111]])의 결과값이 나온다. 
텐서를 사용한 계산에서의 역전파는 
원소별 연산을 수행하는 함수는 입출력 데이터가 스칼라라고 가정하며 순전파와 역전파를 구현할 수 있다.
또한텐서를 입력해도 역전파가 올바르게 성립한다.
원소별로 계산하지 않는 함수는 
1.텐서의 형상을 변환하는 reshape 함수
2.행렬을 전치하는 transpose 함수
3.두 함수 모두 텐서의 형상을 바꾸는 함수 이다.
텐서의 형상을 바꾸는 함수
넘파이의 reshape 함수 사용법은
np.reshape(x, shape) 형태로 쓰며 x를 shape 인수로 지정한 형상으로 변환한다.
x의 형상을 (2, 3)에서 (6,)으로 변환하고
텐서의 원소 수는 같고 형상만 변환한다.
코드식은 
x = np.array([[1.2.3].[4, 5.6]])
y = np.reshape(x. (6.))
print(y)
결과값은
[1 2 3 4 5 6]으로 나온다. 
reshape역전파는
reshape 함수는 형상만 변환하므로 구체적인 계산은 아무것도 하지 않는다.
역전파는 출력 쪽에서 전해지는 기울기를 그대로 입력 쪽으로 흘려보낸다.
기울기의 형상이 입력의 형상과 같아지도록 변환한다.
역전파는 출력 쪽에서부터 기울기를 전달하고 기울기를 x.data.shape 와 x.grad.shape가 일치하도록 변환하고,
- (6,) 인 형상을 (2, 3)으로 변환한다.
DeZero용 reshape 함수 구현하면
1.Reshape 클래스를 초기화할 때 변형 목표가 되는 형상을 shape 인수로 받는다.
2.순전파시에 넘파이의 reshape 함수를 사용하여 형상을 변환한다.
3.self.x_shape = x.shape 코드에서 입력 x의 형상을 기억해 둔다.
4.역전파에서 입력 형상 (self.x_shape)로 변환할 수 있다.
reshape 함수 구현하는 법은
인수 x는 ndarray 인스턴스 또는 Variable 인스턴스이다.
from dezero.core import as variable
det reshape(x, shape):
    if x.shape == shape:
        return as_variable(x)
    return Reshape(shape) (x)
class Reshape(Function):
   def __init__(self, shape): 
      selt.shape shape
   def forward(self, x):
      self.x_shape = x.shape 
      y = x.reshape(self.shape)
      return y
   def backward(self, gy):
      return reshape(gy. self.x_shape)
이다.
그리고 
구현한 reshape 함수 사용법은 
1. reshape 함수를 사용하여 형상을 변환시킨다.
2. y.backward(retain_grad=True)를 수행하여 x의 기울기를 구한다.
3. 이 과정에서 y의 기울기도 자동으로 채워진다.
4. 채워진 기울기의 형상은 y와 같다. (y.grad.shape == y.shape)
5. 원소는 모두 1로 이루어진 텐서이다.
Variable에서 reshape 를 사용할려면 
넘파이의 reshape을 사용한다. 
x = np.random.rand(1.2.3)
y = x.reshape ((2.3))
y = x.reshape([2,3])
y = x.reshape (2.3)
그리고 DeZero의 reshape 함수를 넘파이의 reshape와 비슷하게 만들려면 
Variable 클래스에 가변 인수를 받는 reshape 메서드를 추가한다. 
그리고 reshape 함수를 Variable 인스턴스의 메서드 형태로 호출 할 수 있다. 
행렬을 전치해주는 함수는 
행렬을 전치하면 행렬의 형상이 변한다. 
넘파이의 transpose함수는
transpose 함수를 사용하여 전치를 알 수 있다. 
x의 현상이 (2,3)에서 (3,2)로 변환한다. 
텐서의 원소 자체는 그대로이고 형상만 변환한다, 
또한 역전파에서는 출력 쪽에 전해지는 기울기의 형상만 변환한다, 
또한 순전파때와 정확히 반대 형태이다. 
X = np.array([[1, 2, 3]. [4, 5, 6]])
y = np.transpose(x)
print(y)
하면 결과값은 
[[14]
[25]
[36]]이다.
DeZeor의 transpose의 함수를 구현하면
class Transpose (Function):
    def forward(self. x):
        y = np.transpose(x)
        return y
    def backward(self, gy):
        gx = transpose(gy)
        return gx
def transpose(x): 
    return Transpose()(x)
x = Variable(np.array([[1.2.3]. [4.5.6]]))
y = F.transpose(x)
y.backward()
print(x.grad)
하면 결과값은 
variable([[111] 
         [111]])이다.
Variable 클래스에서 transpose 함수를 추가하는 법은 
1. 두개의 메서드를 추가하고 
2. 첫번째인 transpose는 인스턴스 메서드로 이용하기 위한 코드이며, 
3. 두번째 T에는 @property데코레이터가 붙어 인스턴수 변수로 사용한다. 
def transpose(self):
    return dezero.functions.transpose(self)

@property
def T(self):
    return dezero.functions.transpose(self)
이다.
sum의 합계 함수는 덧셈의 미분이다. 
역전파는 출력쪽에서 전해지는 기울기를 그대로 입력쪽으로 보낸다. 
덧셈을 수행한 후 변수 y로 부터 역전파를 진행한다. 
X0과 X1에는 출력 쪽에서 전해준 1이라는 기울기를 두 개로 복사하여 전달한다. 
원소 2개로 구성된 벡터 합의 역전파는 
벡터에 sum 함수를 적용하면 스칼라를 출력한다. 
그리고 역전파는 출력 쪽에서 전해준 값인 1을 [1,1]이라는 벡터로 확장해 전파한다. 
원소가 2개 이상인 벡터의 합에 대한 역전파는 
1. 기울기 벡터의 원소 수 만큼 복사 한다.
2. 기울기를 입력변수의 형상과 같이 복사한다.
3. 입력 변수가 2차원 이상의 배열일 때도 동일하게 적용된다. 
DeZeor의 Sum클래스의 Sum 함수 구현하는 법은 
Sum함ㅁ수 역전파에서는 입력변수의 형상과 같아지도록 기울기의 원소를 복사한다. 
그리고 지정한 형상에 맞게 원소를 복사하기 위해 broadcas_to 함수를 사용한다, 
그리고 broadcas_to 함수를 ㅏ용하여 입력변수와 형상이 같아지도록 기울기 gy의 원소를 복사한다. 
행렬을 입력하여 벡터가 아닌 경우 동작을 확인한다. 
코드는
class Sum(Function):
    def forward(self. x):
        self.x_shape = x.shape
        y = x.sum()
        return y
    det backward(selt, gy):
        gx = broadcast_to(gy, self.x_shape)
        return ge

def sum(x):
    return Sum()(x)이다.
축 지정 인수는 
1. 합계를 구할 때 축을 지정할 수 있고 
x의 형상은 (2,3)이고 출력 y의 형상은 (3,)이다.
Axis는 축을 뜻하며, 다차원 배열에서 화살표의 방향을 의미한다.
식은
x= np.array([1, 2, 3],[ 14, 5, 6]])
ynp.sum(x. axis = 0)
print(y)
print(x.shape.'->' , y.shape)
이다. 
keepdims 인수는
keepdims 는 입력과 출력의 차원 수(축 수)를
똑같게 유지할지 정하는 플래그이며.
keepdims=True로 지정하면 축의 수가 유지한다.
keepdims=False로 지정하면 y의 형상은 스칼라이다.
DeZero의 sum 함수에서 axis와 keepdims 인수 지원하는 방법은
sum 함수의 역전파에 적용되는 이론은 동일함하며
입력 변수와 형상이 같아지도록 기울기의 원소를 복사한다.
Sum 클래스를 초기화할 때 axis와 keepdims를 입력받아 속성으로 설정한다.
순전파에서 이 속성들을 사용해 합계를 구한다.
역전파시 broadcast to 함수를 사용하여 입력 변수의 형상과 같아지도록 기울기 원소 복사한다.
브로드캐스트 함수는 
1. 넘파이 브로드캐스트
서로 다른 형상(shape)을 가진 배열들간에 산술 연산을 수행하기 위해 배열의 형상을 조정하고
브로드캐스트를 사용하여 작은 배열을 큰 배열에 맞추어 연산을 수행할 수 있다.
2.DeZero에서도 넘파이와 같은 브로드캐스트 지원
sum 함수를 구현시 역전파에서 구현되지 않은 broadcast_to 함수를 이용했고
broadcast_to 함수를 구현한다.
넘파이의 브로드케스트 함수는
넘파이의 np.broadcast_to(x, shape) 함수이며
원래는 (3,) 형상이던 1차원 배열의 원소를 복사하여 (2, 3) 형상으로 바꿈
np.broadcast_to 함수의 역전파
입력 x의 형상과 같아지도록 기울기의 합을 구한다.
sum_to(x, shape) 함수가 있으면 간단하게 해결된다.
넘파이의 sum_to함수를 준비하는 과정은 
파일 위치는 dezero/unils.py을 찾으며ㅡ
sun_to(x,shape)함수는 shape형상이 되도록 합을 계산한다. 
그리고 기능은 np.sum 함수와 같지만 인수를 준느 방법이 다르다. 
sum_to 함수의 역전파는 
역전파는 broadcast_to 함수를 그대로 이용한다. 
broadcast_to 함수를 사용하여 입력x의 형상과 같아지도록 기울기의 원소를 복제한다. 
그리고 
broadcast_to함수와 BroadcastTo클래스 구현한다.
역전파에서는 입력 x 와 형상을 일치시키는데 DeZero의 sum_to함수를 이용한다.
SumTo클래스와 sum_to함수를 구현한다.
 역전파에서는 입력 x와 형상이 같아지도록 기울기의 원소를 복제한다. 
broadcast_to함수와 sum_to 함수는 상호 의존적이다.
Broadcast_To함수 
class BroadcastTo(Function):
    def __init__(self, shape):
        self.shape shape
    def forward(self. x):
        self.x_shape = x.shape
        xp = dezero.cuda.get_array_module(>
        y=xp.broadcast_to(x. self.shape)
        return y
    def backward(self, gy):  
        gx = sum to(gy, self.x shape)
        return gx
def broadcast_to(x, shape):
    if x.shape == shape:
        return as_variable(x)
    return BroadcastTo(shape) (x)

sum_to 함수
class SumTo(Function):
    def __init__(self, shape):
        self.shape = shape
    def forward(self. x):
        self.x_shape = x. shape
        y = utils.sum_to(x. self.shape)
        return y
    def backward(self, gy):
        gx = broadcast_to(gy, self.x_shape)
        return gx
def sum_to(x, shape):
    if x.shape == shape:
        return as_variable(x)
    return SumTo(shane) (x)
브로드캐스트는 형상이 다른 다차원 배열끼리의 연산을 가능하게 하는 넘파이 기능이다.
 역전파의 계산은 
DeZero의 Add 클래스를 수정한다. 
또한 순전파일 때 브로드캐스트가 일어난다면 입력되는 x0과 x1의 형상이 다르다
이점을 이용해 두 형상이 다를 때 브로드캐스트용 역전파를 계산한다. 
기울기는 gx0는 x0의 형상이 되도록 합을 구하고, 기울기 gx1은 x1의 형상이 되도록 합을 구한다. 
다음은 행렬의 곱이다.
벡터의 내적은 
두 벡터 사이의 대응원소의 곱을 모두 합한 값이다. 
행렬의 곱은 왼쪽 행렬의 가로 방향 벡터와 오른쪽 행렬의 세로방향 벡터사이의 내적을 계산한다. 
백터의 내적과 행렬의 곱 계산은 모두 np.dot 함수로 처리 할 수 있다.
행렬과 벡터를 사용한 계산 시 체크 할 점은 
1. shape에 주의를 해야하며
2. 행렬 a와 b의 대응하는 차원의 원소 수가 일치해야 한다.
3. 결과로 만들어진 행렬 c 의 형상은 행렬 a 와 같은 수의 행을, 행렬 b와 같은 수의 열을 갖는다. 
행렬 곱의 역전파는 
최종적으로 스칼라를 출력하는 계산을 다룬다.
행렬 곱을 수행하는 법은
행렬의 원소를 꼐산하여 양 변을 비교하면 유도 된다. 
그릭 행렬 곱의 형상 체크도 충족하는지 확인한다. 
행렬의 곱 코드를 구현하면 
class MatMul (Function):
    def forward(self, x, W):
        y = x.dot (W)
        return y
    def backward(self, gy):
        x, W self.inputs 
        gx = matmul(gy. W.T) 
        gw = matmul(x.T. gy)
        return gx, gW
def matmul(x, W):
    return MatMul() (x, W)이다.
8주차

선형 회귀
머신러닝의 본질은 
데이터를 사용하여 문제를 해결한다.
토이 데이터셋은 실험용으로 만든 작은 데이터 셋이다. 
예측모델은
x와 y라는 두 개의 변수로 구성된 데이터 셋을 생성한다.
x와 y는 선형관계이다.
y에 추가된 노이즈 때문에 점들이 구름처럼 퍼져 있다
x값이 주어지면 y값을 예측하는 모델을 만드는 것이다.

import numpy as np

np.random.seed(0)
x = np.random.rand(100, 1)
y = 5 + 2*x + np.random.rand(100, 1)

모아진 데이터로부터 컴퓨터가 스스로 해결책을 찾아 내는 것이다. 
 무제점 중간 계산값이 존재한다. = 계산시에도 메모리를 차지하고 있기 때문에 메모리 최적화를 위한 코드이다.
mean_square_error 코드를 사용한다. 
신경망
아핀 변환 - 입력 x 와 매개변수 W 사이에서 행렬 곱을 구하고 b를 더하는 것이다. 선형 변화는 신경망에서 완전연결계층
선형 변환을 linear 함수로 구현
복잡한 함수를 linear 함수로 종합시키는것이다. = 그렇기에 사용자가 편하다.
def linear_simple(x, W, b =None):
  t = matmul(x, W)
  if b is None:
    return t
    y = t+b
    t.data = None
    return y
 비선형 데이터 
선형 회귀로는 문제를 풀 수 없다.
그래서 신경망을 이용해야함
 신경망은 비선형 변환을 수행해야한다. 
 선형 변화 > 활성화 함수 > 선형 변환 > 활성화 함수 > ...
서선형 변환과 활성화 함수를 순서대로 적용한다.
추론의 정확성을 높이려면 학습이 필요하다.
손실함수의 출력을 최소화하는 매개변수를 찾는다.
코드
def predict(x):
  y = F.linear(x, W1, b1)
  y = F.sigmoid(y)
  y = F.linear(y, W2, b2)
  return y  
 Layer 클래스 
변수를 변환시켜줌
Fuction과 다른점은 매개변수를 유지함
Parameter 클래스
Parameter 클래스는 Variable 클래스와 똑같은 기능을 갖게 한다.
Variable 클래스를 상속한 것 뿐이라 기능도 Variable 클래스와 동일하다.
▪ Parameter 인스턴스와 Variable 인스턴스 구별하는 법은
Parameter 인스턴스와 Variable 인스턴스를 조합하여 계산할 수 있고
또한 isinstance 함수로 구분할 수 있다.
이 점을 이용하여 Parameter 인스턴스만을 담는 구조를 만들 수 있음
import numpy as np
from dezero import Variable, Parameter

x= Variable(np.array(1.0))
p = Parameter(np.array(2.0))
y = x *p
Layer클래스는
Function 클래스와 마찬가지로 변수를 변환하는 클래스이며,
매개변수를 유지한다는 점이 다르다.
 Layer 클래스를 기반 클래스로 두고 구체적인 변환은 자식 클래스에서 구현한다.
Layer클래스를 구현하면
from dezero.core import Parameter

class Layer:
    def __init__(self):
        self._params = set()
    def __setattr__(self, name, value):
        if isinstance (value, Parameter):
            self._params.add(name)
        super().__setattr_(name, value)
이다.
선형 변환을 하는 Linear 클래스 구현하는 법은 
1. 계층으로서의 Linear 클래스를 구현한다.
2. Linear 클래스는 Layer 클래스를 상속하여 구현한다.
3.  가중치와 편향은 두 Parameter 인스턴스 변수의 이름이 self._params에 추가되고
4.  forward 메서드로 선형 변환을 구현한다.

class Linear (Layer):
    def __init__(self, in_size, out_size, nobias False, dtype-np.float32) 
        super().__init__()

    1.0 in_size, out_size
    W_data np.random.randn(1, 0).astype (dtype) np.sqrt(1/1)
    self.W Parameter(W_data, name='W')
    if nobias:
        self.b = None
    else:
    self.b = Parameter(np.zeros(0, dtype=dtype), name='b')
def forward(self, x):
    y= F.linear (x, self.W, self.b)
    return y
Linear 클래스를 이용하여 신경망을 구현하는 법은
1. 이전 문제 sin 함수의 데이터셋에 대한 회귀 문제를 다시 풀어본다.
2. 주목할 점은 매개변수 관리를 Linear 인스턴스가 맡고 있다.
모델을 표현하기 위한 Model 클래스 생성
신경망도 수식으로 표현할 수 있는 함수이며 그것을 가르켜 모델이라고 한다.
Model 클래스는 Layer 클래스의 상속한다.
시각화를 위한 plot 메서드는 인수 *inputs로 전달받은 데이터를 forward 메서드로 계산 후
생성된 계산 그래프를 이미지 파일로 내보낸다.
Model 클래스는 마치 Layer 클래스처럼 활용할 수 있다.
식은
from dezero import Variable, Model
import dezero.layers as L
import dezero.functions as F

class TwoLayerNet (Model):
    def __init_(self, hidden_size, out_size):
        super().__init__()
        self.11 L.Linear (hidden_size)
        self.12 L.Linear (out_size)
    def forward(self. x):
        y F.sigmoid(self.11(x))
        yself.12(y)
        return y

x = Variable (np.random.randn(5, 10), name='x )
model TwoLayerNet (100, 10)
model.plot(x)
이다.
이 Model 클래스의 사용 문제를 해결할려면 
sin 함수로 생성한 데이터셋 회귀 문제를 Model 클래스를 이용하여 다시 풀어본다.
Model 클래스를 상속한 TwoLayerNet으로 신경망을 구현한다.
모든 매개변수는 model을 통해 접근할 수 있다.
매개변수의 기울기 재설정도 model.cleargrads( )로 처리한다.
class TwoLayerNet (Model):
    def _init(self, hidden size, out size):
        super()._init__()
        self.11 L.Linear (hidden_size)
        self.12 L.Linear(out size)
def forward(self, x):
    y F.sigmoid(self.11(x))
    yself.12(y)
    return y
를
model = TwoLayer Net (hidden_size. 1)

for i in range(max_iter):
    y_pred model (x)
    Toss = F.mean_squared_error(y, y_pred)
    
    model.cleargrads()
    loss.backward()
for p in model.params():
    p.data Irp.grad.data
if i % 1000 == 0:
    print(loss)
로 변경한다.
MLP 클래스 이다. 
완전연결계층 신경망을 구현하는데 
fc_output_sizes는 신경망을 구성하는 완전연결 계층들의 출력 크기를 튜플 또는 리스트로 지정한다. 
계층 모델에 인스턴스 변수를 설정하는 식으로 계층에 포함된 매개 변수들을 관리한다. 
다음은 식이다.
class MLP(Model):
    def __init__(self, fc_output_sizes, activation F.sigmoid): super()._init_()
    self.activation activation
    self.layers = []
    for i, out_size in enumerate(fc_output_sizes): 
        layer L.Linear (out_size) 
        setattr(self. I str(i), layer) 
        self.layers.append(layer)
def forward(self, x):
    for I in self.layers[:-1]: 
        x= self.activation (1(x))
    return self.layers[-1](x)
