Optimizer ë°‘ ë°ì´í„° ì „ì²˜ë¦¬ì— ëŒ€í•´ì„œ 
Optimizer í´ë˜ìŠ¤ëŠ” 
ë§¤ê²Œ ë³€ìˆ˜ ê°±ì‹ ì„ ìœ„í•œ ê¸°ë°˜ í´ë˜ìŠ¤ì´ë‹¤. 
ë˜í•œ êµ¬ì²´ì ì¸ ìµœì í™” ê¸°ë²•ì€ Optimizer í´ë˜ìŠ¤ë¥¼ ìƒì†í•œ ìì‹í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•œë‹¤. 
ì´ˆê¸°í™” ë©”ì„œë“œëŠ” targetê³¼ hooksë¼ëŠ” ë‘ê°œì˜ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•œë‹¤. 
êµ¬ì²´ì ì¸ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ ì€  update_one ë©”ì„œë“œì—ì„œ ìˆ˜í–‰, ìì‹ í´ë˜ì„œì—ì„œ ì¬ì •ì˜ ëœë‹¤. 
ë˜í•œ ì „ì²˜ë¦¬ëŠ” add_hook ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œë‹¤.
ì‹ì€ 
class Optimizer:
  def init (self):
    self.target None
    selt.hooks []

  def setup(self, target):
    self target target
    return self

  def update(self):
    params = [p for p in self.target.params() if p.grad is not None]
    for f in self.hooks:
      f(params)

    for param in params:
      self.update_one(param)

  def update_one(self, param):
    raise NotimplementedError()

  def add_hook(self, f):
    self.hooks.append(f)
SGD í´ë˜ìŠ¤ êµ¬í˜„ì€
ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ëŠ” í´ë´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
SGD í´ë˜ìŠ¤ëŠ” Optimizer í´ë˜ìŠ¤ë¥¼ ìƒì†í•©ë‹ˆë‹¤. 
ë˜í•œ __init__ë©”ì„œë“œëŠ” í•™ìŠµë¥ ì„ ë°›ì•„ ì´ˆê¸°í™”ë¥¼ ì§„í–‰í•œë‹¤. 
update_one ë©”ì„œë“œì—ì„œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹  ì½”ë“œë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. 
êµ¬í˜„ì‹ì€ 
class SGD(Optimizer):
  def __init__(self, Ir=0.01):
    super().__init__()
    self.Ir = Ir
 
  def update_one(self, param):
    param.data = self.Ir param.grad.data
SGD í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒê·€ ë¬¸ì œë¥¼ í’€ë ¤ë©´ 
MLP í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìƒì„±í•˜ê³ 
SGDí´ë˜ìŠ¤ë¡œ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê°±ì‹ í•œë‹¤. optimizer.update()ë¥¼ ì‚¬ìš©í•œë‹¤.
ë‹¤ìŒì€ ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•œ ìµœì í™” ê¸°ë²•ì´ë‹¤. 
Momentum, AdaGrad, AdaDelta, Adamì´ ìˆìœ¼ë©°
OPtimizer í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ì„ í•„ìš”ì— ë”°ë¼ ì†ì‰½ê²Œ ì „í™˜í•œë‹¤. 
Optimizer í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ì—¬ ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ì„ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ë‹¤. 
momentum ê¸°ë²•ì´ë€ 
WëŠ” ê°±ì‹ í•  ê°€ì¤‘ì¹˜ ë§¤ê°œ ë³€ìˆ˜ì´ë©°, ğœ•ğ¿/ğœ•WëŠ” ê¸°ìš¸ê¸°. nì€ í•™ìŠµë¥ ì„ ë§í•œë‹¤. 
vëŠ” ë¬¼ë¦¬ì—ì„œ ë§í•˜ëŠ” ì†ë„ì— í•´ë‹¹í•˜ë©°, avëŠ” ë¬¼ì²´ê°€ ì•„ë¬´ëŸ° í˜ì„ ë°›ì§€ ì•Šì„ë•Œ ì„œì„œíˆ ê°ì†ë˜ëŠ” ì—­í™œì„ í•œë‹¤.
MomentumSGD êµ¬í˜„ ì½”ë“œëŠ” 
ì†ë„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°, ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì˜ ì¸ìŠ¤í„´ìˆ˜ ë³€ìˆ˜ self.vsì— ìœ ì§€ê°€ ë˜ëŠ” ê²ƒì´ë©° 
ì´ˆê¸°í™” ì‹œì—ëŠ” vsì— ì•„ë¬´ê²ƒë„ ë‹´ê²¨ ìˆì§€ ì•Šë‹¤. 
ë˜í•œ Update_one()ì´ ì²˜ìŒ í˜¸ì¶œë  ë•Œ ë§¤ê°œë³€ìˆ˜ì™€ ê°™ì€ íƒ€ì…ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤. 
êµ¬í˜„í•œ í•™ìŠµì½”ë“œì—ì„œ ì†ì‰½ê²Œ Momentumìœ¼ë¡œ ì „í™˜ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 
class MomentumSGD(Optimizer):
  def __init__(self. Ir=0.01. momentum = 0.9):
    super().__init__()
    self.Ir = Ir
    self.momentum = momentum
    self.vs = {}
  
  def update_one(self, param):
    v_key id(param)
    if v_key not in self.vs:
      xp = cuda.get_array_module(param.data) 
      self.vs[v_key] xp.zeros_like (param.data)
    
    v = self.vs[v_key]
    v *= self.momentum
    v self.Ir param.grad.data
    param.data += v
ì´ê³³ì—ì„œ optimizer = MomentumSGD(lr)ë¡œ ë³€ê²½í•˜ë©´ ëœë‹¤. 
